Regular expressions:

  * strings with a special syntax
  * allow us to match patterns in other strings
  * applications of regular expressions:
      > find all weblinks in a document
      > parse email addresses, remove/replace unwanted characters
      
  In[]: import re
  
  In[]: re.match('abc', 'abcdef')
  Out[]: <_sre.SRE_Match object; span=(0,3), match='abc'>
  
  In[]: word_regex = '\w+'
  
  In[]: re.match(word_regex, 'hi there!')
  Out[]: <_sre.SRE_Match object; span=(0,2), match='hi'>
  
  * common regex patterns:
        PATTERN       MATCHES         EXAMPLE
        
        \w+           word             'Magic'
        \d            digit             9
        \s            space            ''
        .*            wildcard         'username74'
        + or *        greedy match     'aaaaa'
        \S            not space        'no_spaces'
        [a-z]         lowercase group  'abcdefg'
        
  * Python's re module:
      > split: split a string on regex
      > findall: find all patterns in a string
      > search: search for a pattern
      > match: match an entring string or substring based on a pattern
      
      In regex library, the rule is to pass the pattern first and the string second.
      It may return an iterator, string or match object.
      
      In[]: re.split('\s+', 'split on spaces.')
      Out[]: ['split', 'on', 'spaces.']
      
      In[]: re.findall(r'\w+', "Let's write RegEx!")
      Out[]: ['Let', 's', 'write', 'RegEx']
      
      It's important to prefix regex patterns with r to ensure that your patterns are interpreted in the way 
      you want them to. Else, we may encounter problems to do with escape sequences in strings. 
      For example, "\n" in Python is used to indicate a new line, but if you use the r prefix,
      it will be interpreted as "\n".
      
  * EXERCISE
      1)
        my_string = "Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  
                     Or perhaps, all 19 words?"
                     
        # Import the regular expression module re.
          In[]: import re
          
        # Split my_string on each sentence ending. To do this:
        # Write a pattern called sentence_endings to match sentence endings (., !, and ?).
          In[]: sentence_endings = r"[.?!]"
          
        # Use re.split() to split my_string on the pattern and print the result.
          In[]: print(re.split(sentence_endings, my_string))
          Out[]: ["Let's write RegEx", "  Won't that be fun", '  I sure think so', '  Can you find 4 sentences', 
                 '  Or perhaps, all 19 words', '']
       
        # Find all capitalized words in my_string by writing a pattern called capitalized_words
        # and using re.findall(). Print the result.
          In[]: capitalized_words = r"[A-Z]\w+"
          In[]: print(re.findall(capitalized_words, my_string))
          Out[]: ['Let', 'RegEx', 'Won', 'Can', 'Or']
          
        # Write a pattern called spaces to match spaces and then use re.split() to split my_string on this pattern,
        # keeping all punctuation intact. Print the result.
          In[]: spaces = r"\s+"
          In[]: print(re.split(spaces, my_string))
          Out[]: ["Let's", 'write', 'RegEx!', "Won't", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 
                  'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']

        # Find all digits in my_string by writing a pattern called digits and using re.findall(). Print the result.
          In[]: digits = r"\d+"
          In[]: print(re.findall(digits, my_string))
          Out[]: ['4', '19']
          
  * TOKENIZATION:
      
      > turning a string or document into smaller chunks called tokens
      > eg's: 
          # breaking out words or sentences
          # separating punctuation
          # separating all hastags in a tweet
      > nltk library - natural language toolkit
        In[]: from nltk.tokenize import word_tokenize
        
        In[]: word_tokenize("Hi there!")
        Out[]: ['Hi', 'there', '!']
        
      > Why tokenize?
        # matching common words
        # removing unwanted tokens
        Eg "I don't like sam's shoes"
           "I", "do", "n't", "like", "sam", "'s", "shoes", "."
      
      > other nltk tokenizers:
        
        # sent_tokenize: tokenize a document into sentences
        # regexp_tokenize: token a string or document based on regular expression pattern
        # TweetTokenizer: special class just for tweet tokenization, allowing us to separate hashtags, mentions and lots
                          of exclamation points
        
      > difference between re.search() and re.match()
      
          In[]: import re
          
          In[]: re.match('abc', 'abcde')
          Out[]: <_sre.SRE_Match object; span=(0,3), match='abc'>
          
          In[]: re.search('abc', 'abcde')
          Out[]: <_sre.SRE_Match object; span=(0,3), match='abc'>
          
          In[]: re.match('cd', 'abcde')
          
          In[]: re.search('cd', 'abcde')
          Out[]: <_sre.SRE_Match object; span=(2,4), match='cd'>
  
  * EXERCISE
    
    1)
      > scene_one = "SCENE 1: [wind] [clop clop clop] 
                     KING ARTHUR: Whoa there!  [clop clop clop] 
                     SOLDIER #1: Halt!  Who goes there?
                     ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  
                             King of the Britons, defeator of the Saxons, sovereign of all England!
                     SOLDIER #1: Pull the other one!
                     ARTHUR: I am, ...  and this is my trusty servant Patsy.
                             We have ridden the length and breadth of the land in search of knights 
                             who will join me in my court at Camelot.  I must speak with your lord and master.
                    SOLDIER #1: What?  Ridden on a horse?
                    ARTHUR: Yes!
                    SOLDIER #1: You're using coconuts!
                    ARTHUR: What?
                    SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.
                    ARTHUR: So?  We have ridden since the snows of winter covered this land, 
                            through the kingdom of Mercea, through--
                    SOLDIER #1: Where'd you get the coconuts?
                    ARTHUR: We found them.
                    SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!
                    ARTHUR: What do you mean?
                    SOLDIER #1: Well, this is a temperate zone.
                    ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek 
                             warmer climes in winter, yet these are not strangers to our land?
                    SOLDIER #1: Are you suggesting coconuts migrate?
                    ARTHUR: Not at all.  They could be carried.
                    SOLDIER #1: What?  A swallow carrying a coconut?
                    ARTHUR: It could grip it by the husk!
                    SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!
                                A five ounce bird could not carry a one pound coconut.
                    ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur 
                            from the Court of Camelot is here.
                    SOLDIER #1: Listen.  In order to maintain air-speed velocity,
                                a swallow needs to beat its wings forty-three times every second, right?
                    ARTHUR: Please!
                    SOLDIER #1: Am I right?
                    ARTHUR: I'm not interested!
                    SOLDIER #2: It could be carried by an African swallow!
                    SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.
                    SOLDIER #2: Oh, yeah, I agree with that.
                    ARTHUR: Will you ask your master if he wants to join my court at Camelot?!
                    SOLDIER #1: But then of course a-- African swallows are non-migratory.
                    SOLDIER #2: Oh, yeah...
                    SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] 
                    SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?
                    SOLDIER #1: No, they'd have to have it on a line.
                    SOLDIER #2: Well, simple!  They'd just use a strand of creeper!
                    SOLDIER #1: What, held under the dorsal guiding feathers?
                    SOLDIER #2: Well, why not?"
      
      > Import the sent_tokenize and word_tokenize functions from nltk.tokenize.
        In[]: from nltk.tokenize import sent_tokenize
        In[]: from nltk.tokenize import word_tokenize
        
      > Tokenize all the sentences in scene_one.
        In[]: sentences = sent_tokenize(scene_one)
        
      > Tokenize the fourth sentence in sentences.
        In[]: tokenized_sent = word_tokenize(sentences[3])
        
      > Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set.
        In[]: unique_tokens = set(word_tokenize(scene_one))
        
      > Print the unique tokens found.
        In[]: print(unique_tokens)
        Out[]: {'needs', 'may', 'Who', 'found', '[', 'King', 'held', 'guiding', ']', 'using', 'European', 'Halt', "'",
                'second', 'maybe', 'he', 'coconut', 'me', 'Pull', 'migrate', 'question', 'velocity', 'A', "'s", 'our',
                'just', 'In', 'where', 'my', 'plover', 'strand', 'wants', 'in', 'Are', 'Arthur', 'Listen', 'why', 'goes',
                'Camelot', 'Mercea', 'times', 'climes', 'Well', '.', 'court', 'fly', 'ratios', 'Oh', "'em", 'non-migratory',
                'bangin', 'here', 'its', 'with', 'I', '#', 'Patsy', 'and', 'yet', 'suggesting', 'England', 'through', 'get',
                'warmer', 'African', 'husk', 'Uther', 'creeper', 'of', 'a', 'simple', 'zone', 'horse', 'empty', 'weight',
                'got', 'castle', 'Wait', 'do', 'will', 'at', 'grip', 'must', '1', 'minute', '?', 'They', 'by', 'sovereign',
                'Not', 'or', "'m", 'No', 'back', 'The', 'matter', 'not', 'bird', 'all', 'Will', 'go', 'Whoa', 'defeator',
                'SOLDIER', 'tropical', 'that', 'but', 'then', 'winter', ',', 'house', "n't", 'your', 'mean', 'Ridden', 'two',
                'line', 'together', 'this', 'tell', '2', 'beat', 'trusty', 'every', 'are', 'other', 'That', 'Yes', 'swallows'
                , 'one', 'seek', 'Court', 'it', '!', 'What', 'Found', 'wings', 'lord', 'servant', 'pound', 'Pendragon',
                'there', 'Supposing', '--', "'d", 'knights', 'yeah', 'under', 'right', 'coconuts', ':', 'SCENE', 'We',
                'ask', 'maintain', 'wind', 'south', 'halves', 'clop', 'join', 'to', 'search', 'agree', 'on', 'be', 'speak',
                'if', 'breadth', 'order', 'ounce', 'ridden', 'covered', 'the', 'them', 'does', 'But', 'an', 'Where', 'use',
                'land', 'these', 'feathers', 'anyway', 'air-speed', 'carried', 'course', 'temperate', 'Britons', 'snows',
                'bring', "'ve", 'martin', 'forty-three', 'So', 'swallow', 'KING', 'who', 'am', 'kingdom', 'carrying',
                '...', 'from', 'Am', 'ARTHUR', 'strangers', 'five', 'son', 'Please', 'point', 'they', 'master', "'re",
                'have', 'You', 'grips', 'is', 'you', 'could', 'length', 'Saxons', 'carry', 'dorsal', 'interested', 'sun',
                'since', 'It'}
    2)
      
      The tokenized sentences of scene_one are available in your namespace as sentences
      
      > Print the start and end indexes for the first occurence of the word "coconuts" in scene_one
        In[]: match = re.search("coconuts", scene_one)
        In[]: print(match.start(), match.end())
        Out[]: 580 588
        
      > Write a regular expression called pattern1 to find anything in square brackets.
        In[]: pattern1 = r"\[.*\]"
      
      > Use re.search() with the previous pattern to find the first text in square brackets in the scene.
        In[]: re.search(pattern1, scene_one)
      
      > Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result.
        In[]: pattern2 = r"[\w\s]+:"
        In[]: print(re.match(pattern2, sentences[3]))
        Out[]: <_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>
        
  * ADVANCED TOKENIZATION WITH REGEX
  
      > Regex groups using OR
        
        # OR is represented using |
        # We can define a group using ()
        # we can define explicit character ranges using []
        #  
          In[]: import re
          In[]: match_digits_and_words = ('(\d+|\w+)')
          In[]: re.findall(match_digits_and_words, 'He has 11 cats.')
          Out[]: ['He', 'has', '11', 'cats']
          
      > Regex ranges and groups
      
        PATTERN               MATCHES                                       EXAMPLE
      
        [A-Za-z]+             upper and lowercase english alphabet            'ABCDEFghijk'
        [0-9]                 numbers from 0 to 9                             9
        [A-Za-z\-\.]+         upper and lowercase english alphabet, - and .   'My-Website.com'
        (a-z)                 a, - and z                                      'a-z'
        (\s+|,)               spaces or a comma                               ','
        
        
        Character range with re.match()
        
        In[]: import re
        
        In[]: my_str = 'match lowercase spaces nums like 12, but no commas'
        
        In[]: re.match('[a-z0-9 ]+', my_str)
        Out[]: <_sre.SRE_Match object; span=(0, 42), match='match lowercase spaces nums like 12'>
        
        
  * EXERCISE
    1)
      > Twitter is a frequently used source for NLP text and tasks.The nltk.tokenize.TweetTokenizer
        class gives us some extra methods and attributes for parsing tweets. We're given some example 
        tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. 
        These example tweets have been pre-loaded into the variable tweets.
        tweets = ['This is the best #nlp exercise ive found online! #python',
                  '#NLP is super fun! <3 #learning',
                  'Thanks @datacamp :) #nlp #python']
        
      > Import the regexp_tokenize and TweetTokenizer from nltk.tokenize
        In[]: from nltk.tokenize import regexp_tokenize
        In[]: from nltk.tokenize import TweetTokenizer
      
      > Define a regex pattern called pattern1 to match hashtags.
        Then, call regexp_tokenize() with new hashtag pattern on the first tweet in tweets.
        In[]: pattern1 = r"#\w+"
        In[]: regexp_tokenize(tweets[0], pattern1)
      
      > Write a new pattern called pattern2 to match mentions and hashtags.
        Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets
        In[]: pattern2 = r"([#|@]\w+)"
        In[]: regexp_tokenize(tweets[-1], pattern2)
        
      > Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize 
        each tweet into a new list called all_tokens
        In[]: tknzr = TweetTokenizer()
        In[]: all_tokens = [tknzr.tokenize(t) for t in tweets]
        In[]: print(all_tokens)
        Out[]: [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], 
                ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], 
                ['Thanks', '@datacamp', ':)', '#nlp', '#python']]
                
    2)
      > german_text = "Wann gehen wir zum Pizza? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•"
        The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.
        Unicode ranges for emoji are: ('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), 
        ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF').
      
      > Tokenize all the words in german_text
        In[]: word_tokenize(german_text)
      
      > Tokenize only the capital words in german_text.Note German -> Ãœ
        In[]: capital_words = r"[A-ZÃœ]\w+"
        In[]: print(regexp_tokenize(german_text, capital_words))
        Out[]: ['Wann', 'Pizza', 'Und', 'Ãœber']
      
      > Tokenize only the emoji in german_text
        In[]: emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
        print(regexp_tokenize(german_text, emoji))
        Out[]: ['ðŸ•', 'ðŸš•']
       
  * CHARTING WORD LENGTH WITH NLTK
    
    > Combining NLP data extraction with plotting
      In[]: from matplotlib import pyplot as plt
      In[]: from nltk.tokenize import word_tokenize
      In[]: words = word_tokenize("This is a pretty cool tool!")
      In[]: word_lengths = [len(w) for w in words]
      In[]: plt.hist(word_lengths)
      Out[]: (array([ 2.,  0.,  1.,  0.,  0.,  0.,  3.,  0.,  0.,  1.]),
              array([ 1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,  5.5,  6. ]),
              <a list of 10 Patch objects>)
      Out[]: plt.show()
      # a word length histogram is generated
  
  * EXERCISE
    1) 
      > The Holy Grail script is loaded for us, and we need to use regex to find the words per line.
      
      > Split the script into lines using the newline ('\n') character.
        In[]: lines = holy_grail.split('\n')
        
      > Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1
        In[]: pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
        In[]: lines = [re.sub(pattern, '', l) for l in lines]
      
      > Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words
        In[]: tokenized_lines = [regexp_tokenize(s, "\w+") for s in lines]
      
      > Use a list comprehension to create a list of line lengths called line_num_words
        In[]: line_num_words = [len(t_line) for t_line in tokenized_lines]
      
      > Plot a histogram of line_num_words using plt.hist()
        In[]: plt.hist(line_num_words)
        In[]: plt.show()
     
